{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. (wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset\n",
    "\n",
    "In here we are using <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">Large Movie Review Dataset</a> from Stanford. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. This dataset provides a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import glob\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading the dataset\n",
    "def load_dataset(data_dir = \"./data/imdb-reviews/\"):\n",
    "\n",
    "    # Initializing a dictionary for X_data and y_data\n",
    "    X_data = {}\n",
    "    y_data = {}\n",
    "    \n",
    "    # Iterating through the \"train\" and \"test\"\n",
    "    for train_or_test in ['train', 'test']:\n",
    "        \n",
    "        # Initialize an empty dictionary for the train and test\n",
    "        X_data[train_or_test] = {}\n",
    "        y_data[train_or_test] = {}\n",
    "        \n",
    "        # Iterate through \"positive\", \"negative\"\n",
    "        for positive_or_negative in ['positive', 'negative']:\n",
    "            \n",
    "            # Initialize an empty list for each sentiment\n",
    "            X_data[train_or_test][positive_or_negative] = []\n",
    "            y_data[train_or_test][positive_or_negative] = []\n",
    "            \n",
    "            # Get the name of all texts in our folder\n",
    "            file_names = glob.glob(os.path.join(data_dir, train_or_test, positive_or_negative, \"*.txt\")) \n",
    "                \n",
    "            # Iterate through file names\n",
    "            for i_file in file_names:\n",
    "                \n",
    "                # Open the (text) file\n",
    "                with open(i_file) as i_file:\n",
    "                \n",
    "                    # Assign values to our dictionary from that file\n",
    "                    X_data[train_or_test][positive_or_negative].append(i_file.read())\n",
    "                    y_data[train_or_test][positive_or_negative].append(positive_or_negative)\n",
    "                \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "X_data, y_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " 12500 Positive / 12500 Negative\n",
      "\n",
      "Test set:\n",
      " 12500 Positive / 12500 Negative\n"
     ]
    }
   ],
   "source": [
    "# Get the shape dataset\n",
    "print(\"Training set:\\n {} Positive / {} Negative\\n\".format(len(X_data[\"train\"][\"positive\"]), \n",
    "                                                           len(X_data[\"train\"][\"negative\"])))\n",
    "\n",
    "print(\"Test set:\\n {} Positive / {} Negative\".format(len(X_data[\"test\"][\"positive\"]), \n",
    "                                                   len(X_data[\"test\"][\"negative\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and test set\n",
    "X_train = X_data[\"train\"][\"positive\"] + X_data[\"train\"][\"negative\"]\n",
    "y_train = y_data[\"train\"][\"positive\"] + y_data[\"train\"][\"negative\"]\n",
    "\n",
    "X_test = X_data[\"test\"][\"positive\"] + X_data[\"test\"][\"negative\"]\n",
    "y_test = y_data[\"test\"][\"positive\"] + y_data[\"test\"][\"negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suffling the trianing set and test ste\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_test, y_test = shuffle(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set = 25000 \n",
      "Test set = 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set = {} \\nTest set = {}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a small subset of dataset (for speed purposes)\n",
    "X_train, y_train = X_train[:4000], y_train[:4000]\n",
    "X_test, y_test = X_test[:1000], y_test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "At the second step, We will prerpocess our dataset which is an essential part of any type of model. More specifically we will apply the following steps for preprocessing:\n",
    "1. Lowercasing the text\n",
    "2. Removing the punctuation\n",
    "3. Converting to tokens\n",
    "4. Removing the stopwords\n",
    "5. Apply stemmer\n",
    "6. Apply lemmizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from keras.preprocessing import sequence\n",
    "import bs4\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Removing all HTML tags\n",
    "    text = bs4.BeautifulSoup(text, \"html5lib\").get_text().strip()\n",
    "    \n",
    "    # Lowercasing the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing the punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "    # Converting to tokens\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing the stopwords\n",
    "    tokens = [i_token for i_token in tokens if i_token not in stopwords.words(\"english\")]\n",
    "\n",
    "    # Apply stemmer\n",
    "    stemmed = [PorterStemmer().stem(i_token) for i_token in tokens]\n",
    "\n",
    "    # Apply lemmizer\n",
    "    lemmtized = [WordNetLemmatizer().lemmatize(i_token, pos=\"n\") for i_token in stemmed]\n",
    "    lemmtized = [WordNetLemmatizer().lemmatize(i_token, pos=\"v\") for i_token in lemmtized]\n",
    "\n",
    "    return lemmtized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproces the training set and test set\n",
    "X_train = [preprocess_text(i) for i in X_train]\n",
    "X_test = [preprocess_text(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total dataset\n",
    "total_dataset = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2id and id2word\n",
    "all_unique_words = np.unique([item for sub_list in total_dataset for item in sub_list])\n",
    "word2id = {i_token: index for index, i_token in enumerate(all_unique_words)}\n",
    "id2word = {index: i_token for index, i_token in enumerate(all_unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping words in training to its corresponding id\n",
    "for index, sub_list in enumerate(X_train):\n",
    "    X_train[index] = list(map(lambda x: word2id[x], sub_list))\n",
    "    \n",
    "# Mapping words in test set to its corresponding id\n",
    "for index, sub_list in enumerate(X_test):\n",
    "    X_test[index] = list(map(lambda x: word2id[x], sub_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequence\n",
    "max_words = 500\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert labels into 0 and 1\n",
    "def str_to_int(label):\n",
    "    if label == \"positive\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "y_train = list(map(str_to_int, y_train))\n",
    "y_test = list(map(str_to_int, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "Now we are ready to feed the data into our model for training. As you will see, Even with a simple architecture we can reach to a high accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Reshape\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hyperparameters\n",
    "embedding_size = 32\n",
    "lstm_units = 100\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "vocabulary_size = len(all_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           831360    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 884,661\n",
      "Trainable params: 884,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length = max_words))\n",
    "model.add(LSTM(units = lstm_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 119s 30ms/step - loss: 0.6959 - acc: 0.5410 - val_loss: 0.6726 - val_acc: 0.5670\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67256, saving model to ./saved model/weights.best.sentiment_analysis.hdf5\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 109s 27ms/step - loss: 0.6322 - acc: 0.7300 - val_loss: 0.5844 - val_acc: 0.7700\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67256 to 0.58436, saving model to ./saved model/weights.best.sentiment_analysis.hdf5\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 121s 30ms/step - loss: 0.3782 - acc: 0.8752 - val_loss: 0.3958 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58436 to 0.39578, saving model to ./saved model/weights.best.sentiment_analysis.hdf5\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 119s 30ms/step - loss: 0.1643 - acc: 0.9470 - val_loss: 0.3696 - val_acc: 0.8480\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39578 to 0.36961, saving model to ./saved model/weights.best.sentiment_analysis.hdf5\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 120s 30ms/step - loss: 0.0735 - acc: 0.9795 - val_loss: 0.3954 - val_acc: 0.8400\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.36961\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 120s 30ms/step - loss: 0.0342 - acc: 0.9925 - val_loss: 0.4386 - val_acc: 0.8470\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.36961\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 120s 30ms/step - loss: 0.0226 - acc: 0.9948 - val_loss: 0.5179 - val_acc: 0.8460\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.36961\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 108s 27ms/step - loss: 0.0130 - acc: 0.9982 - val_loss: 0.6540 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.36961\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 108s 27ms/step - loss: 0.0059 - acc: 0.9990 - val_loss: 0.6062 - val_acc: 0.8350\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.36961\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 104s 26ms/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.6709 - val_acc: 0.8330\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.36961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5ca71dd8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint for saving the model\n",
    "checkpointer = ModelCheckpoint(filepath='./saved model/weights.best.sentiment_analysis.hdf5', \n",
    "                               verbose = 1, \n",
    "                               save_best_only = True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, \n",
    "          y_train,\n",
    "          validation_data = (X_test, y_test),\n",
    "          batch_size = batch_size,\n",
    "          epochs = num_epochs,\n",
    "          callbacks = [checkpointer], \n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "Once you have trained your model, it's time to see how well it performs on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 83.3%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
    "print(\"Test Set Accuracy: {}%\".format(scores[1]*100))  # scores[1] should correspond to accuracy if you passed in metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction\n",
    "\n",
    "Now you are ready for prediction. You can check the sentiment of any sentence you input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for prediction\n",
    "def text_to_predict(unseen_text):\n",
    "    \n",
    "    # Preprocess the text\n",
    "    unseen_text = preprocess_text(unseen_text)\n",
    "    \n",
    "    # Convert the words to ids\n",
    "    unseen_text = list(map(lambda x: word2id[x], unseen_text))\n",
    "    \n",
    "    # Pad sequences\n",
    "    unseen_text = sequence.pad_sequences([unseen_text], max_words)\n",
    "    \n",
    "    # Get the prediction\n",
    "    prediction = model.predict(unseen_text)[0][0]*100\n",
    "\n",
    "    # Print the result\n",
    "    if prediction < 0.5:\n",
    "        print(\"The given sentence is negative.\")\n",
    "    elif prediction > 0.5:\n",
    "        print(\"The given sentence is positive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given sentence is positive.\n"
     ]
    }
   ],
   "source": [
    "# Predict a unseen text\n",
    "unseen_text = \"The movie is absolutely terrible. It's not something i would suggest\"\n",
    "text_to_predict(unseen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given sentence is positive.\n"
     ]
    }
   ],
   "source": [
    "# Predict a unseen text\n",
    "unseen_text = \"The movie is absolutely great. Can't wait to watch it again.\"\n",
    "text_to_predict(unseen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESOURCES:**\n",
    "1. <a href=\"https://monkeylearn.com/sentiment-analysis/\">Sentiment Analysis - nearly everything you need to know</a>\n",
    "2. <a href=\"https://medium.freecodecamp.org/how-to-make-your-own-sentiment-analyzer-using-python-and-googles-natural-language-api-9e91e1c493e\">How to make your own sentiment analyzer using Python and Googleâ€™s Natural Language API</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#  Topic Modeling\n",
    "\n",
    "In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model. (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset\n",
    "\n",
    "At the very start, We will load the dataset and take a look at it. The dataset that we are using, Contains data of news headlines published over a period of 15 years. It sourced from the reputable Australian news source ABC (Australian Broadcasting Corp.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(10101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_csv(filepath_or_buffer = \"./dataset/abcnews-date-text.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the second column\n",
    "documents = data[[\"headline_text\"]]\n",
    "\n",
    "# Get the first 90K Rows only\n",
    "documents = documents[:90000]\n",
    "\n",
    "# Adding the index column\n",
    "documents['index'] = documents.index\n",
    "\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  90000\n"
     ]
    }
   ],
   "source": [
    "# Total number of documents\n",
    "print(\"Total number of documents: \", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Dataset\n",
    "\n",
    "At the second step, We will prerpocess our dataset which is an essential part of any type of model. More specifically we will apply the following steps for preprocessing:\n",
    "1. Lowercasing the text\n",
    "2. Removing the punctuation\n",
    "3. Converting to tokens\n",
    "4. Removing the stopwords\n",
    "5. Apply stemmer\n",
    "6. Apply lemmizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/soheilmohammadpour/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading wordnet \n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Lowercasing the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing the punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "    # Converting to tokens\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing the stopwords\n",
    "    tokens = [i_token for i_token in tokens if i_token not in stopwords.words(\"english\")]\n",
    "\n",
    "    # Apply stemmer\n",
    "    stemmed = [PorterStemmer().stem(i_token) for i_token in tokens]\n",
    "\n",
    "    # Apply lemmizer\n",
    "    lemmtized = [WordNetLemmatizer().lemmatize(i_token, pos=\"n\") for i_token in stemmed]\n",
    "    lemmtized = [WordNetLemmatizer().lemmatize(i_token, pos=\"v\") for i_token in lemmtized]\n",
    "\n",
    "    return lemmtized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the whole document\n",
    "processed_documents = documents['headline_text'].map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [aba, decid, commun, broadcast, licenc]\n",
       "1                  [act, fire, wit, must, awar, defam]\n",
       "2            [g, call, infrastructur, protect, summit]\n",
       "3            [air, nz, staff, aust, strike, pay, rise]\n",
       "4        [air, nz, strike, affect, australian, travel]\n",
       "5                   [ambiti, olsson, win, tripl, jump]\n",
       "6               [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, four, memphi, m...\n",
       "8            [aust, address, un, secur, council, iraq]\n",
       "9                   [australia, lock, war, timet, opp]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_documents.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "\n",
    "### 3.1. Bag-of-Words\n",
    "\n",
    "The bag of words (BoW) model is a simplifying representation used in natural language processing and information retrieval. In this model, a text is represented as the bag of its words, disregarding grammar and even word order but keeping multiplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dictionary cotaining words and their integer ids\n",
    "dictionary = gensim.corpora.Dictionary(processed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 aba\n",
      "1 broadcast\n",
      "2 commun\n",
      "3 decid\n",
      "4 licenc\n",
      "5 act\n",
      "6 awar\n",
      "7 defam\n",
      "8 fire\n",
      "9 must\n"
     ]
    }
   ],
   "source": [
    "# Printing first 10 items in dictionary\n",
    "count = 0\n",
    "for key, value in dictionary.iteritems():\n",
    "    print(key, value)\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletting very rare and very common words\n",
    "dictionary.filter_extremes(no_below = 15, # Removing words with less than 15 (absolute number)\n",
    "                           no_above = 0.1, # Words appearing more than 10% of documents (fraction of size, not absolute number)\n",
    "                           keep_n = 100000) # keeping the most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words of our sample document:  [(40, 1), (43, 1), (47, 1), (48, 1), (49, 1), (50, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Applying Bag-of-Words for each document (a list of words)\n",
    "bow_corpus = [dictionary.doc2bow(document = doc) for doc in processed_documents]\n",
    "\n",
    "print(\"Bag-of-Words of our sample document: \", bow_corpus[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"iraq\" with word id of 40 repeated 1 times\n",
      "The word \"australia\" with word id of 43 repeated 1 times\n",
      "The word \"10\" with word id of 47 repeated 1 times\n",
      "The word \"aid\" with word id of 48 repeated 1 times\n",
      "The word \"contribut\" with word id of 49 repeated 1 times\n",
      "The word \"million\" with word id of 50 repeated 1 times\n"
     ]
    }
   ],
   "source": [
    "# Printing out the BoW for our sample document\n",
    "bow_10 = bow_corpus[10]\n",
    "\n",
    "for i in range(len(bow_10)):\n",
    "    print('The word \"{}\" with word id of {} repeated {} times'.format(dictionary[bow_10[i][0]],\n",
    "                                                                      bow_10[i][0],\n",
    "                                                                      bow_10[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. TF-IDF\n",
    "\n",
    "One limitation of BoW is that it treats every word as being equally important. Even though some words occur frequently within a corpus. The solution is to first count the number of documents in which each word occur (this is called document frequency). Then we have to divide the term frequency by the document frequency of that term. Now we have a metric that is proportional to the frequency of occurrence of a term in a document. And inversely promotional to the number of documents it appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TF-IDF model\n",
    "tdidf = models.TfidfModel(corpus = bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st TD-IDF:  [(0, 0.5215718389702217), (1, 0.4870042588063656), (2, 0.3430208344700119), (3, 0.41760004211431156), (4, 0.44579881184600006)]\n"
     ]
    }
   ],
   "source": [
    "# Applying TF-IDF to the entire corpus\n",
    "tdidf_corpus = tdidf[bow_corpus]\n",
    "\n",
    "print(\"1st TD-IDF: \", tdidf_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5215718389702217),\n",
      " (1, 0.4870042588063656),\n",
      " (2, 0.3430208344700119),\n",
      " (3, 0.41760004211431156),\n",
      " (4, 0.44579881184600006)]\n"
     ]
    }
   ],
   "source": [
    "# Preview TF-IDF for first document\n",
    "for doc in tdidf_corpus:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LDA Model\n",
    "\n",
    "In the previous topic, We applied LDA using bag-of-words feature extraction. In this part, We will do the same steps but using TF-IDF feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LDA model\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus = bow_corpus, \n",
    "                                             num_topics = 10, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 2,\n",
    "                                             workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.023*\"hospit\" + 0.015*\"worker\" + 0.013*\"work\" + 0.013*\"union\" + 0.012*\"find\" + 0.011*\"strike\" + 0.010*\"protest\" + 0.009*\"olymp\" + 0.008*\"bodi\" + 0.008*\"rail\" \n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.021*\"lead\" + 0.019*\"win\" + 0.016*\"elect\" + 0.013*\"take\" + 0.012*\"high\" + 0.011*\"coast\" + 0.011*\"aussi\" + 0.010*\"india\" + 0.010*\"south\" + 0.010*\"shoot\" \n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.015*\"reject\" + 0.015*\"new\" + 0.015*\"top\" + 0.014*\"law\" + 0.013*\"open\" + 0.012*\"claim\" + 0.010*\"leav\" + 0.009*\"feder\" + 0.009*\"final\" + 0.009*\"port\" \n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.021*\"plan\" + 0.020*\"cup\" + 0.018*\"world\" + 0.013*\"support\" + 0.010*\"battl\" + 0.010*\"prepar\" + 0.010*\"park\" + 0.009*\"go\" + 0.008*\"final\" + 0.008*\"ahead\" \n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.027*\"govt\" + 0.014*\"public\" + 0.013*\"help\" + 0.012*\"say\" + 0.011*\"time\" + 0.010*\"urg\" + 0.009*\"opposit\" + 0.009*\"develop\" + 0.009*\"leader\" + 0.009*\"report\" \n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.056*\"polic\" + 0.042*\"man\" + 0.033*\"charg\" + 0.029*\"court\" + 0.028*\"face\" + 0.016*\"death\" + 0.015*\"murder\" + 0.014*\"miss\" + 0.014*\"probe\" + 0.013*\"drug\" \n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.019*\"group\" + 0.018*\"water\" + 0.017*\"wa\" + 0.016*\"council\" + 0.016*\"meet\" + 0.015*\"health\" + 0.014*\"plan\" + 0.013*\"servic\" + 0.012*\"farmer\" + 0.010*\"urg\" \n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.028*\"boost\" + 0.015*\"new\" + 0.015*\"sydney\" + 0.014*\"industri\" + 0.012*\"cut\" + 0.012*\"studi\" + 0.010*\"price\" + 0.010*\"look\" + 0.009*\"doubt\" + 0.009*\"show\" \n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.033*\"call\" + 0.014*\"school\" + 0.014*\"road\" + 0.012*\"want\" + 0.012*\"govt\" + 0.011*\"nsw\" + 0.011*\"crash\" + 0.010*\"accid\" + 0.010*\"council\" + 0.010*\"fine\" \n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.056*\"u\" + 0.029*\"iraq\" + 0.029*\"kill\" + 0.020*\"attack\" + 0.014*\"bomb\" + 0.012*\"trade\" + 0.011*\"blast\" + 0.011*\"iraqi\" + 0.011*\"minist\" + 0.011*\"troop\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploring words in each topic and it weight\n",
    "for index, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(index, topic), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction\n",
    "\n",
    "Now that our model has been trained, Let's predict one sentence in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_doc = \"big plan to boost paroo water supplies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text:  ['aviat', 'world', 'face', 'moment', 'reckon', '737', 'max', 'crash']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the new document\n",
    "processed_text = preprocess_text(text = unseen_doc)\n",
    "print(\"Processed text: \", processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words for given text:  [(206, 1), (427, 1), (429, 1), (1805, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Creating a Bag-of-Word\n",
    "bow_vector = dictionary.doc2bow(document = processed_text)\n",
    "print(\"Bag-of-Words for given text: \", bow_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 41.97% | Topic: 0.056*\"polic\" + 0.042*\"man\" + 0.033*\"charg\"\n",
      "\n",
      "Score: 22.39% | Topic: 0.021*\"plan\" + 0.020*\"cup\" + 0.018*\"world\"\n",
      "\n",
      "Score: 21.63% | Topic: 0.019*\"group\" + 0.018*\"water\" + 0.017*\"wa\"\n",
      "\n",
      "Score: 2.00% | Topic: 0.033*\"call\" + 0.014*\"school\" + 0.014*\"road\"\n",
      "\n",
      "Score: 2.00% | Topic: 0.023*\"hospit\" + 0.015*\"worker\" + 0.013*\"work\"\n",
      "\n",
      "Score: 2.00% | Topic: 0.021*\"lead\" + 0.019*\"win\" + 0.016*\"elect\"\n",
      "\n",
      "Score: 2.00% | Topic: 0.028*\"boost\" + 0.015*\"new\" + 0.015*\"sydney\"\n",
      "\n",
      "Score: 2.00% | Topic: 0.056*\"u\" + 0.029*\"iraq\" + 0.029*\"kill\"\n",
      "\n",
      "Score: 2.00% | Topic: 0.027*\"govt\" + 0.014*\"public\" + 0.013*\"help\"\n",
      "\n",
      "Score: 2.00% | Topic: 0.015*\"reject\" + 0.015*\"new\" + 0.015*\"top\"\n"
     ]
    }
   ],
   "source": [
    "# Checking which topic does our document belongs to\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {:.2f}% | Topic: {}\".format(score*100, lda_model_tfidf.print_topic(topicno = index, topn = 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESOURCES:**\n",
    "1. <a href=\"https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\">Topic Modelling in Python with NLTK and Gensim</a>\n",
    "2. <a href=\"https://towardsdatascience.com/the-complete-guide-for-topics-extraction-in-python-a6aaa6cedbbc\">An overview of topics extraction in Python with LDA</a>\n",
    "3. <a href=\"https://nlpforhackers.io/topic-modeling/\">Complete Guide to Topic Modeling</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
